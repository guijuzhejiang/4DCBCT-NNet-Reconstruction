{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Net医療CT画像処理 - トレーニングノートブック\n",
    "\n",
    "## 概要\n",
    "このノートブックは4D-CBCT画像の伪影除去を目的としたN-Netモデルのトレーニングを実行します。\n",
    "\n",
    "## 実行手順\n",
    "1. **環境準備**: ライブラリのインポートとプロジェクト設定\n",
    "2. **実験設定**: 再現性確保と実験ディレクトリ作成\n",
    "3. **ハードウェア設定**: GPU/CPU使用設定とメトリクス初期化\n",
    "4. **ロギング設定**: TensorBoard・W&B設定\n",
    "5. **データ準備**: データセット作成とローダー設定\n",
    "6. **モデル構築**: N-Netアーキテクチャの初期化\n",
    "7. **最適化設定**: オプティマイザー・損失関数設定\n",
    "8. **トレーニング実行**: メインループとバリデーション\n",
    "9. **結果保存**: ベストモデル保存と可視化\n",
    "10. **リソース解放**: メモリとセッションクリーンアップ\n",
    "\n",
    "## 注意事項\n",
    "- GPU メモリ不足時は batch_size を調整してください\n",
    "- AMP (Automatic Mixed Precision) を使用してメモリ効率を改善\n",
    "- キャッシュ率は使用可能メモリに応じて調整可能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境準備・ライブラリインポート\n",
    "\n",
    "**目的**: プロジェクトルートの設定と必要なライブラリのインポート\n",
    "- プロジェクトルートをPythonパスに追加\n",
    "- 医療画像処理・深層学習関連ライブラリをインポート\n",
    "- 警告メッセージの抑制設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# プロジェクトルートの設定と確認\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"\"))\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"../..\"))\n",
    "\n",
    "# Pythonパスに追加（重複チェック）\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"プロジェクトルートをPythonパスに追加: {project_root}\")\n",
    "else:\n",
    "    print(f\"プロジェクトルートは既にPythonパスに存在: {project_root}\")\n",
    "\n",
    "# ディレクトリ存在確認\n",
    "if not os.path.exists(project_root):\n",
    "    raise FileNotFoundError(f\"プロジェクトルートが見つかりません: {project_root}\")\n",
    "\n",
    "project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 11:09:49.083077: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-03 11:09:49.083114: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-03 11:09:49.084174: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-03 11:09:49.090491: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-03 11:09:50.020209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "\n",
    "# MONAI関連ライブラリ\n",
    "from monai.losses import SSIMLoss, PerceptualLoss\n",
    "from monai.utils import set_determinism\n",
    "from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, ScaleIntensityd, ToTensord\n",
    "from monai.data import CacheDataset, ThreadDataLoader\n",
    "from monai.metrics import SSIMMetric, MAEMetric, PSNRMetric, RMSEMetric\n",
    "\n",
    "# カスタムモジュール\n",
    "from NNet.model_Nnet import Nnet\n",
    "from NNet.Monai.TrainDataset_Nnet import Nnet_Dataset\n",
    "from NNet.config import TRAINING_CONFIG, DATASET_CONFIG, MODEL_CONFIG, LOGGING_CONFIG, SCHEDULER_CONFIG, DEVICE_CONFIG\n",
    "from NNet.utils import get_dataset_slice_counts, setup_device, save_model\n",
    "\n",
    "# 警告を非表示に\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 実験設定・再現性の確保\n",
    "\n",
    "**目的**: 実験の再現性確保と結果管理のためのセットアップ\n",
    "- **再現性**: シード値を固定してトレーニング結果を一貫させる\n",
    "- **実験管理**: タイムスタンプ付きディレクトリで実験結果を整理\n",
    "- **設定保存**: 実験パラメータを自動的にバックアップして再現可能性を確保"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "実験ディレクトリ作成: experiments/Nnet/20250703_103903\n",
      "設定ファイル保存先: experiments/Nnet/20250703_103903/configs\n"
     ]
    }
   ],
   "source": [
    "# 再現性のためのシード設定\n",
    "set_determinism(seed=42)\n",
    "\n",
    "# 実験ディレクトリの作成\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_dir = os.path.join(\"experiments\", \"Nnet\", f\"{timestamp}\")\n",
    "os.makedirs(experiment_dir, exist_ok=True)\n",
    "print(f\"実験ディレクトリ作成: {experiment_dir}\")\n",
    "\n",
    "# 設定の保存\n",
    "def save_configs(experiment_dir):\n",
    "    \"\"\"実験ディレクトリに設定を保存\"\"\"\n",
    "    config_dir = os.path.join(experiment_dir, \"configs\")\n",
    "    os.makedirs(config_dir, exist_ok=True)\n",
    "\n",
    "    configs = {\n",
    "        \"training_config\": TRAINING_CONFIG,\n",
    "        \"dataset_config\": DATASET_CONFIG,\n",
    "        \"model_config\": MODEL_CONFIG,\n",
    "        \"logging_config\": LOGGING_CONFIG,\n",
    "        \"scheduler_config\": SCHEDULER_CONFIG,\n",
    "        \"device_config\": DEVICE_CONFIG\n",
    "    }\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        with open(os.path.join(config_dir, f\"{name}.txt\"), \"w\") as f:\n",
    "            f.write(f\"{name}:\\n\")\n",
    "            for key, value in config.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "    print(f\"設定ファイル保存先: {config_dir}\")\n",
    "\n",
    "save_configs(experiment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. デバイスの設定\n",
    "使用するデバイス（GPU/CPU）を設定し、メトリクス計算器を初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "Memory: 25.4 GB\n",
      "使用デバイス: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# デバイスの設定\n",
    "device = setup_device(DEVICE_CONFIG['use_cuda'], DEVICE_CONFIG['cuda_device'])\n",
    "\n",
    "# メトリクス計算器の初期化\n",
    "ssim_metric = SSIMMetric(spatial_dims=2, reduction=\"mean\")\n",
    "mae_metric = MAEMetric(reduction=\"mean\")\n",
    "psnr_metric = PSNRMetric(max_val=1.0, reduction=\"mean\")\n",
    "rmse_metric = RMSEMetric(reduction=\"mean\")\n",
    "\n",
    "print(f\"使用デバイス: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ロギングの設定\n",
    "TensorBoardとWeights & Biasesの設定を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoardログ保存先: experiments/Nnet/20250703_103903/tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguiju\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>experiments/Nnet/20250703_103903/wandb/run-20250703_104018-h3pmeqlb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/guiju/nnet-medical-ct/runs/h3pmeqlb' target=\"_blank\">20250703_103903</a></strong> to <a href='https://wandb.ai/guiju/nnet-medical-ct' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/guiju/nnet-medical-ct' target=\"_blank\">https://wandb.ai/guiju/nnet-medical-ct</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/guiju/nnet-medical-ct/runs/h3pmeqlb' target=\"_blank\">https://wandb.ai/guiju/nnet-medical-ct/runs/h3pmeqlb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights & Biasesロギングを開始\n"
     ]
    }
   ],
   "source": [
    "# TensorBoard設定\n",
    "if LOGGING_CONFIG['use_tensorboard']:\n",
    "    tb_log_dir = os.path.join(experiment_dir, \"tensorboard\")\n",
    "    os.makedirs(tb_log_dir, exist_ok=True)\n",
    "    tb_writer = SummaryWriter(tb_log_dir)\n",
    "    print(f\"TensorBoardログ保存先: {tb_log_dir}\")\n",
    "else:\n",
    "    tb_writer = None\n",
    "\n",
    "# Weights & Biases設定\n",
    "use_wandb = False\n",
    "if LOGGING_CONFIG['use_wandb']:\n",
    "    run_name = os.path.basename(experiment_dir)\n",
    "    try:\n",
    "        wandb.init(\n",
    "            project=LOGGING_CONFIG['wandb_project'],\n",
    "            entity=LOGGING_CONFIG['wandb_entity'],\n",
    "            config={\n",
    "                **TRAINING_CONFIG,\n",
    "                **DATASET_CONFIG,\n",
    "                **MODEL_CONFIG,\n",
    "                **SCHEDULER_CONFIG\n",
    "            },\n",
    "            name=run_name,\n",
    "            dir=experiment_dir\n",
    "        )\n",
    "        use_wandb = True\n",
    "        print(\"Weights & Biasesロギングを開始\")\n",
    "    except Exception as e:\n",
    "        print(f\"Weights & Biasesの初期化に失敗: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. データセット準備・データローダー作成\n",
    "\n",
    "**目的**: 医療CT画像データの効率的な読み込みと前処理\n",
    "- **データ形式**: PNG画像ファイル（NPY形式にも対応可能）\n",
    "- **前処理**: スケール正規化、チャンネル次元確保、テンソル変換\n",
    "- **キャッシュ戦略**: メモリ使用量を考慮した部分キャッシュ（10%）\n",
    "- **並列処理**: マルチワーカーによる高速データロード\n",
    "\n",
    "**注意**: NPY形式を使用する場合は `TrainDataset_Nnet_NPY.py` と `LoadImaged(reader=\"numpyreader\")` を使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ変換パイプラインの定義（PNG用、NPY用にも対応可能）\n",
    "train_val_transforms = Compose([\n",
    "    LoadImaged(keys=[\"img\", \"prior\", \"label\"]),  # NPY使用時: reader=\"numpyreader\"を追加\n",
    "    EnsureChannelFirstd(keys=[\"img\", \"prior\", \"label\"]),  # チャンネル次元を最初に配置\n",
    "    ScaleIntensityd(keys=[\"img\", \"prior\", \"label\"]),      # [0,1]範囲に正規化\n",
    "    ToTensord(keys=[\"img\", \"prior\", \"label\"])             # PyTorchテンソルに変換\n",
    "])\n",
    "\n",
    "# データルートパスの絶対パス化\n",
    "DATASET_CONFIG['data_root'] = os.path.join(project_root, DATASET_CONFIG['data_root'])\n",
    "print(f\"データルート: {DATASET_CONFIG['data_root']}\")\n",
    "\n",
    "# トレーニングデータセットの準備\n",
    "print(\"トレーニングデータセットを準備中...\")\n",
    "slice_counts_train = get_dataset_slice_counts(\n",
    "    DATASET_CONFIG['data_root'],\n",
    "    DATASET_CONFIG['train_dataset_indices'],\n",
    ")\n",
    "print(f\"対象データセット: {DATASET_CONFIG['train_dataset_indices']}\")\n",
    "print(f\"トレーニングスライス数: {slice_counts_train}\")\n",
    "\n",
    "train_dataset = Nnet_Dataset(\n",
    "    DATASET_CONFIG['data_root'],  # 降質画像ルート\n",
    "    DATASET_CONFIG['data_root'],  # 先験画像ルート\n",
    "    DATASET_CONFIG['data_root'],  # GT画像ルート\n",
    "    DATASET_CONFIG['train_dataset_indices'],\n",
    "    slice_counts_train,\n",
    ")\n",
    "\n",
    "# バリデーションデータセットの準備\n",
    "print(\"\\nバリデーションデータセットを準備中...\")\n",
    "slice_counts_val = get_dataset_slice_counts(\n",
    "    DATASET_CONFIG['data_root'],\n",
    "    DATASET_CONFIG['val_dataset_indices'],\n",
    ")\n",
    "print(f\"対象データセット: {DATASET_CONFIG['val_dataset_indices']}\")\n",
    "print(f\"バリデーションスライス数: {slice_counts_val}\")\n",
    "\n",
    "val_dataset = Nnet_Dataset(\n",
    "    DATASET_CONFIG['data_root'],\n",
    "    DATASET_CONFIG['data_root'],\n",
    "    DATASET_CONFIG['data_root'],\n",
    "    DATASET_CONFIG['val_dataset_indices'],\n",
    "    slice_counts_val,\n",
    ")\n",
    "\n",
    "# メモリ効率を考慮したキャッシュデータセットの作成\n",
    "print(\"\\nキャッシュデータセットを作成中...\")\n",
    "# 利用可能メモリに応じてcache_rateを調整可能（0.1 = 10%キャッシュ）\n",
    "cache_rate = 0.1  \n",
    "print(f\"キャッシュ率: {cache_rate*100}% (メモリ使用量を抑制)\")\n",
    "\n",
    "train_dataset_cache = CacheDataset(\n",
    "    data=train_dataset.samples,\n",
    "    transform=train_val_transforms,\n",
    "    cache_rate=cache_rate,\n",
    "    num_workers=4,       # 並列処理ワーカー数\n",
    "    progress=True        # プログレスバー表示\n",
    ")\n",
    "\n",
    "val_dataset_cache = CacheDataset(\n",
    "    data=val_dataset.samples,\n",
    "    transform=train_val_transforms,\n",
    "    cache_rate=cache_rate,\n",
    "    num_workers=2,\n",
    "    progress=True\n",
    ")\n",
    "\n",
    "# 高効率データローダーの作成\n",
    "print(\"\\nデータローダーを作成中...\")\n",
    "train_loader = ThreadDataLoader(\n",
    "    train_dataset_cache,\n",
    "    batch_size=TRAINING_CONFIG['train_batch_size'],\n",
    "    shuffle=True,                    # トレーニング時はシャッフル\n",
    "    num_workers=TRAINING_CONFIG['num_workers'],\n",
    "    drop_last=False,                 # 最後の不完全バッチも使用\n",
    "    pin_memory=torch.cuda.is_available(),  # CUDA使用時はメモリ固定\n",
    "    prefetch_factor=2,               # プリフェッチバッファサイズ\n",
    "    persistent_workers=True          # ワーカープロセス永続化\n",
    ")\n",
    "\n",
    "val_loader = ThreadDataLoader(\n",
    "    val_dataset_cache,\n",
    "    batch_size=TRAINING_CONFIG['val_batch_size'],\n",
    "    num_workers=TRAINING_CONFIG['num_workers'],\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    prefetch_factor=2\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 データセット統計:\")\n",
    "print(f\"  トレーニングサンプル数: {len(train_dataset_cache):,}\")\n",
    "print(f\"  バリデーションサンプル数: {len(val_dataset_cache):,}\")\n",
    "print(f\"  トレーニングバッチ数: {len(train_loader)}\")\n",
    "print(f\"  バリデーションバッチ数: {len(val_loader)}\")\n",
    "print(f\"  バッチサイズ (train/val): {TRAINING_CONFIG['train_batch_size']}/{TRAINING_CONFIG['val_batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. モデルの構築\n",
    "N-netモデルを構築し、デバイスに移動します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Netモデルの初期化とデバイス配置\n",
    "print(\"N-Netモデルを初期化中...\")\n",
    "model = Nnet()\n",
    "model = model.to(device)\n",
    "\n",
    "# モデルパラメータ数の確認\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"📋 モデル情報:\")\n",
    "print(f\"  総パラメータ数: {total_params:,}\")\n",
    "print(f\"  訓練可能パラメータ数: {trainable_params:,}\")\n",
    "print(f\"  モデルサイズ: {total_params * 4 / 1024**2:.1f} MB (float32)\")\n",
    "\n",
    "# モデルアーキテクチャの詳細表示\n",
    "print(\"\\n🏗️ モデルアーキテクチャ詳細:\")\n",
    "try:\n",
    "    summary(model, [\n",
    "        (MODEL_CONFIG['input_channels'], *DATASET_CONFIG['image_size']),  # 降質画像入力\n",
    "        (MODEL_CONFIG['input_channels'], *DATASET_CONFIG['image_size'])   # 先験画像入力\n",
    "    ])\n",
    "except Exception as e:\n",
    "    print(f\"アーキテクチャ表示エラー: {str(e)}\")\n",
    "    print(\"モデル構造の簡略表示:\")\n",
    "    print(model)\n",
    "\n",
    "# TensorBoardへのモデルグラフ追加（エラーハンドリング付き）\n",
    "if tb_writer:\n",
    "    try:\n",
    "        print(\"\\n📊 TensorBoardにモデルグラフを追加中...\")\n",
    "        # ダミー入力の作成（入力サイズに合わせて）\n",
    "        dummy_input1 = torch.randn(\n",
    "            1, MODEL_CONFIG['input_channels'], *DATASET_CONFIG['image_size']\n",
    "        ).to(device)\n",
    "        dummy_input2 = torch.randn(\n",
    "            1, MODEL_CONFIG['input_channels'], *DATASET_CONFIG['image_size']\n",
    "        ).to(device)\n",
    "        \n",
    "        # モデルグラフをTensorBoardに追加\n",
    "        tb_writer.add_graph(model, (dummy_input1, dummy_input2))\n",
    "        print(\"✅ モデルグラフをTensorBoardに正常に追加\")\n",
    "        \n",
    "        # メモリ解放\n",
    "        del dummy_input1, dummy_input2\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ TensorBoardグラフ追加に失敗: {str(e)}\")\n",
    "\n",
    "print(\"\\n✅ モデル構築完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. オプティマイザーと損失関数の設定\n",
    "トレーニングに必要なオプティマイザー、スケジューラー、損失関数を設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング設定の計算と表示\n",
    "batches_per_epoch = len(train_loader)\n",
    "total_batches = TRAINING_CONFIG['epochs'] * batches_per_epoch\n",
    "print(f\"📈 トレーニング設定:\")\n",
    "print(f\"  エポック数: {TRAINING_CONFIG['epochs']}\")\n",
    "print(f\"  バッチ数/エポック: {batches_per_epoch}\")\n",
    "print(f\"  総バッチ数: {total_batches:,}\")\n",
    "\n",
    "# Adamオプティマイザーの設定\n",
    "print(f\"\\n⚙️ オプティマイザー設定:\")\n",
    "print(f\"  種類: Adam\")\n",
    "print(f\"  初期学習率: {SCHEDULER_CONFIG['max_lr']}\")\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=SCHEDULER_CONFIG['max_lr'],\n",
    "    betas=(0.9, 0.999),           # Adam momentum parameters\n",
    "    eps=1e-8,                     # numerical stability\n",
    "    weight_decay=0                # L2正則化（必要に応じて調整）\n",
    ")\n",
    "\n",
    "# コサイン学習率スケジューラーの設定\n",
    "print(f\"\\n📉 学習率スケジューラー設定:\")\n",
    "print(f\"  種類: CosineAnnealingLR\")\n",
    "print(f\"  最大学習率: {SCHEDULER_CONFIG['max_lr']}\")\n",
    "print(f\"  最小学習率: {SCHEDULER_CONFIG['min_lr']}\")\n",
    "print(f\"  総ステップ数: {total_batches:,}\")\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_batches,          # 全トレーニングステップ数\n",
    "    eta_min=SCHEDULER_CONFIG['min_lr'],  # 最小学習率\n",
    "    last_epoch=-1                 # 初期値\n",
    ")\n",
    "\n",
    "# 医療画像専用損失関数の設定\n",
    "print(f\"\\n🎯 損失関数設定:\")\n",
    "print(f\"  複合損失関数（4つの損失の重み付き組み合わせ）\")\n",
    "\n",
    "# 各損失関数の初期化\n",
    "mse_loss = MSELoss().to(device)\n",
    "l1_loss = L1Loss().to(device)\n",
    "ssim_loss = SSIMLoss(spatial_dims=2).to(device)\n",
    "perceptual_loss = PerceptualLoss(spatial_dims=2, network_type=\"alex\").to(device)\n",
    "\n",
    "# 損失重みの設定（config.pyから取得）\n",
    "loss_weights = {\n",
    "    'mse': TRAINING_CONFIG['weight_mse'],       # 画像再構築の基本損失\n",
    "    'l1': TRAINING_CONFIG['weight_l1'],         # エッジ保持のためのL1損失\n",
    "    'ssim': TRAINING_CONFIG['weight_ssim'],     # 構造類似性損失\n",
    "    'perceptual': TRAINING_CONFIG['weight_percep']  # 知覚的損失\n",
    "}\n",
    "\n",
    "print(f\"  損失重み: MSE={loss_weights['mse']}, L1={loss_weights['l1']}, SSIM={loss_weights['ssim']}, Perceptual={loss_weights['perceptual']}\")\n",
    "\n",
    "# 医療画像特化の複合損失関数\n",
    "def combined_loss(pred, target):\n",
    "    \"\"\"\n",
    "    4つの損失関数を組み合わせた医療画像専用損失\n",
    "    \n",
    "    Args:\n",
    "        pred: モデル予測結果 [B, C, H, W]\n",
    "        target: 正解画像 [B, C, H, W]\n",
    "    \n",
    "    Returns:\n",
    "        combined_loss: 重み付き損失の合計\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 各損失の計算\n",
    "        mse = mse_loss(pred, target)          # Mean Squared Error\n",
    "        l1 = l1_loss(pred, target)            # L1 Loss (MAE)\n",
    "        ssim = ssim_loss(pred, target)        # Structural Similarity\n",
    "        percep = perceptual_loss(pred, target) # Perceptual Loss\n",
    "        \n",
    "        # 重み付きで組み合わせ\n",
    "        total_loss = (loss_weights['mse'] * mse + \n",
    "                     loss_weights['l1'] * l1 + \n",
    "                     loss_weights['perceptual'] * percep + \n",
    "                     loss_weights['ssim'] * ssim)\n",
    "        \n",
    "        return total_loss\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"損失計算エラー: {str(e)}\")\n",
    "        # フォールバック: MSE損失のみ\n",
    "        return mse_loss(pred, target)\n",
    "\n",
    "print(f\"\\n✅ 最適化設定完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. メトリクス計算関数\n",
    "バリデーション用のメトリクス計算関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(pred, target):\n",
    "    \"\"\"\n",
    "    医療画像評価指標の効率的な計算\n",
    "    \n",
    "    医療画像の品質評価に重要な4つの指標を計算します：\n",
    "    - RMSE: 根平均二乗誤差（画素レベルの再構築精度）\n",
    "    - MAE: 平均絶対誤差（平均的な画素差）\n",
    "    - PSNR: ピーク信号対雑音比（画像品質の客観評価）\n",
    "    - SSIM: 構造類似性指数（人間の視覚的認識に近い評価）\n",
    "    \n",
    "    Args:\n",
    "        pred: モデル予測結果 [B, C, H, W]\n",
    "        target: 正解画像 [B, C, H, W]\n",
    "    \n",
    "    Returns:\n",
    "        dict: 各評価指標の値を含む辞書\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # データ型の統一（計算精度向上のため）\n",
    "        pred = pred.float()\n",
    "        target = target.float()\n",
    "        \n",
    "        # 各メトリクスのリセット（MONAI特有の要件）\n",
    "        ssim_metric.reset()\n",
    "        mae_metric.reset()\n",
    "        psnr_metric.reset()\n",
    "        rmse_metric.reset()\n",
    "        \n",
    "        # バッチ単位でのメトリクス計算\n",
    "        # SSIM: 構造類似性（-1〜1、1が最良）\n",
    "        ssim_metric(pred, target)\n",
    "        ssim_val = ssim_metric.aggregate().item()\n",
    "        \n",
    "        # MAE: 平均絶対誤差（0が最良）\n",
    "        mae_metric(pred, target)\n",
    "        mae_val = mae_metric.aggregate().item()\n",
    "        \n",
    "        # PSNR: ピーク信号対雑音比（高いほど良い、通常20-50dB）\n",
    "        psnr_metric(pred, target)\n",
    "        psnr_val = psnr_metric.aggregate().item()\n",
    "        \n",
    "        # RMSE: 根平均二乗誤差（0が最良）\n",
    "        rmse_metric(pred, target)\n",
    "        rmse_val = rmse_metric.aggregate().item()\n",
    "        \n",
    "        return {\n",
    "            \"rmse\": rmse_val,\n",
    "            \"mae\": mae_val,\n",
    "            \"psnr\": psnr_val,\n",
    "            \"ssim\": ssim_val\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ メトリクス計算エラー: {str(e)}\")\n",
    "        # エラー時のフォールバック値\n",
    "        return {\n",
    "            \"rmse\": float('inf'),\n",
    "            \"mae\": float('inf'),\n",
    "            \"psnr\": 0.0,\n",
    "            \"ssim\": 0.0\n",
    "        }\n",
    "\n",
    "def format_metrics_string(metrics_dict, precision=6):\n",
    "    \"\"\"\n",
    "    メトリクス辞書を見やすい文字列に変換\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict: calculate_metricsの返り値\n",
    "        precision: 小数点以下の桁数\n",
    "    \n",
    "    Returns:\n",
    "        str: フォーマット済み文字列\n",
    "    \"\"\"\n",
    "    return (f\"RMSE: {metrics_dict['rmse']:.{precision}f}, \"\n",
    "            f\"MAE: {metrics_dict['mae']:.{precision}f}, \"\n",
    "            f\"PSNR: {metrics_dict['psnr']:.{precision}f}dB, \"\n",
    "            f\"SSIM: {metrics_dict['ssim']:.{precision}f}\")\n",
    "\n",
    "print(\"✅ メトリクス計算関数を定義完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. メイントレーニングループ実行\n",
    "\n",
    "**目的**: N-Netモデルの効率的なトレーニング実行\n",
    "- **トレーニング**: フォワード・バックワードパス、勾配最適化\n",
    "- **バリデーション**: 過学習監視とベストモデル保存\n",
    "- **ログ記録**: TensorBoard・W&Bでの詳細なメトリクス追跡\n",
    "- **メモリ管理**: OOM対応とGPUメモリ効率化\n",
    "- **進捗表示**: リアルタイムでの学習状況可視化\n",
    "\n",
    "**重要パラメータ**:\n",
    "- 勾配クリッピング: `max_norm=1.0` （勾配爆発防止）\n",
    "- プログレス表示: 10バッチごと（計算効率とログ詳細度のバランス）\n",
    "- ベストモデル保存: バリデーション損失最小時に自動保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トレーニングを開始します...\n",
      "\n",
      "エポック 1/40\n",
      "--------------------------------------------------\n",
      "[エポック 1] トレーニング中\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# メトリクスの計算\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m batch_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 進捗表示\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# 10バッチごとに表示\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(pred, target)\u001b[0m\n\u001b[1;32m     11\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 各メトリクスの計算\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mssim_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m ssim_val \u001b[38;5;241m=\u001b[39m ssim_metric\u001b[38;5;241m.\u001b[39maggregate()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m mae_metric(pred, target)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_DL_cu118/lib/python3.10/site-packages/monai/metrics/metric.py:347\u001b[0m, in \u001b[0;36mCumulativeIterationMetric.__call__\u001b[0;34m(self, y_pred, y, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m, y_pred: TensorOrList, y: TensorOrList \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m Sequence[torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m Sequence[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m    Execute basic computation for model prediction and ground truth.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    It can support  both `list of channel-first Tensor` and `batch-first Tensor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m        a `batch-first` tensor (BC[HWD]) or a list of `batch-first` tensors.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;241m*\u001b[39mret)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_DL_cu118/lib/python3.10/site-packages/monai/metrics/metric.py:80\u001b[0m, in \u001b[0;36mIterationMetric.__call__\u001b[0;34m(self, y_pred, y, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y_pred, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     79\u001b[0m     y_ \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred or y must be a list/tuple of `channel-first` Tensors or a `batch-first` Tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_DL_cu118/lib/python3.10/site-packages/monai/metrics/regression.py:85\u001b[0m, in \u001b[0;36mRegressionMetric._compute_tensor\u001b[0;34m(self, y_pred, y)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred and y must be PyTorch Tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_shape(y_pred, y)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_DL_cu118/lib/python3.10/site-packages/monai/metrics/regression.py:310\u001b[0m, in \u001b[0;36mSSIMMetric._compute_metric\u001b[0;34m(self, y_pred, y)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_dims \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dims \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred should have 5 dimensions (batch, channel, height, width, depth) when using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m spatial dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m ssim_value_full_image, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_ssim_and_cs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspatial_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspatial_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_sigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_sigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m ssim_per_batch: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m ssim_value_full_image\u001b[38;5;241m.\u001b[39mview(ssim_value_full_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    324\u001b[0m )\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssim_per_batch\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_DL_cu118/lib/python3.10/site-packages/monai/metrics/regression.py:412\u001b[0m, in \u001b[0;36mcompute_ssim_and_cs\u001b[0;34m(y_pred, y, spatial_dims, kernel_size, kernel_sigma, data_range, kernel_type, k1, k2)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kernel_type \u001b[38;5;241m==\u001b[39m KernelType\u001b[38;5;241m.\u001b[39mUNIFORM:\n\u001b[1;32m    410\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((num_channels, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mkernel_size)) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mprod(torch\u001b[38;5;241m.\u001b[39mtensor(kernel_size))\n\u001b[0;32m--> 412\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_dst_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    414\u001b[0m c1 \u001b[38;5;241m=\u001b[39m (k1 \u001b[38;5;241m*\u001b[39m data_range) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# stability constant for luminance\u001b[39;00m\n\u001b[1;32m    415\u001b[0m c2 \u001b[38;5;241m=\u001b[39m (k2 \u001b[38;5;241m*\u001b[39m data_range) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# stability constant for contrast\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_DL_cu118/lib/python3.10/site-packages/monai/utils/type_conversion.py:388\u001b[0m, in \u001b[0;36mconvert_to_dst_type\u001b[0;34m(src, dst, dtype, wrap_sequence, device, safe)\u001b[0m\n\u001b[1;32m    386\u001b[0m     output_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(dst)\n\u001b[1;32m    387\u001b[0m output: NdarrayTensor\n\u001b[0;32m--> 388\u001b[0m output, _type, _device \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_data_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrap_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, monai\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mMetaTensor):\n\u001b[1;32m    392\u001b[0m     output\u001b[38;5;241m.\u001b[39mcopy_meta_from(dst)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_DL_cu118/lib/python3.10/site-packages/monai/utils/type_conversion.py:330\u001b[0m, in \u001b[0;36mconvert_data_type\u001b[0;34m(data, output_type, device, dtype, wrap_sequence, safe)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(output_type, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    329\u001b[0m     track_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28missubclass\u001b[39m(output_type, monai\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mMetaTensor)\n\u001b[0;32m--> 330\u001b[0m     data_ \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrap_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_, orig_type, orig_device\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(output_type, np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "File \u001b[0;32m~/miniconda3/envs/py310_DL_cu118/lib/python3.10/site-packages/monai/utils/type_conversion.py:160\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(data, dtype, device, wrap_sequence, track_meta, safe)\u001b[0m\n\u001b[1;32m    158\u001b[0m dtype \u001b[38;5;241m=\u001b[39m get_equivalent_dtype(dtype, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# skip array of string classes and object, refer to:\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/blob/v1.9.0/torch/utils/data/_utils/collate.py#L13\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SaUO]\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;66;03m# numpy array with 0 dims is also sequence iterable,\u001b[39;00m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;66;03m# `ascontiguousarray` will add 1 dim if img has no dim, so we only apply on data with dims\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ベストモデルの追跡変数\n",
    "best_val_loss = float('inf')\n",
    "best_val_metrics = None\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"トレーニングを開始します...\")\n",
    "\n",
    "for epoch in range(TRAINING_CONFIG['epochs']):\n",
    "    print(f\"\\nエポック {epoch + 1}/{TRAINING_CONFIG['epochs']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # --- トレーニングフェーズ ---\n",
    "    model.train()\n",
    "    print(f\"[エポック {epoch + 1}] トレーニング中\")\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # データをデバイスに移動\n",
    "        images = batch[\"img\"].to(device)\n",
    "        prior = batch[\"prior\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        # 勾配のリセット\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            # フォワードパス\n",
    "            prediction = model(images, prior)\n",
    "            loss = combined_loss(prediction, labels)\n",
    "            \n",
    "            # バックワードパスとパラメータ更新\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # メトリクスの計算\n",
    "            batch_metrics = calculate_metrics(prediction, labels)\n",
    "            \n",
    "            # 進捗表示\n",
    "            if (i + 1) % 10 == 0:  # 10バッチごとに表示\n",
    "                print(f\"[エポック {epoch+1} バッチ {i+1}] 損失: {loss.item():.6f}, \"\n",
    "                      f\"RMSE: {batch_metrics['rmse']:.6f}, MAE: {batch_metrics['mae']:.6f}\")\n",
    "            \n",
    "            # TensorBoardロギング\n",
    "            if tb_writer:\n",
    "                global_step = epoch * len(train_loader) + i\n",
    "                tb_writer.add_scalar('Train/Loss', loss.item(), global_step)\n",
    "                tb_writer.add_scalar('Train/RMSE', batch_metrics['rmse'], global_step)\n",
    "                tb_writer.add_scalar('Train/MAE', batch_metrics['mae'], global_step)\n",
    "                tb_writer.add_scalar('Train/PSNR', batch_metrics['psnr'], global_step)\n",
    "                tb_writer.add_scalar('Train/SSIM', batch_metrics['ssim'], global_step)\n",
    "            \n",
    "            # Weights & Biasesロギング\n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    'train_loss': loss.item(),\n",
    "                    'train_rmse': batch_metrics['rmse'],\n",
    "                    'train_mae': batch_metrics['mae'],\n",
    "                    'train_psnr': batch_metrics['psnr'],\n",
    "                    'train_ssim': batch_metrics['ssim'],\n",
    "                    'epoch': epoch + 1\n",
    "                })\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(\"警告: メモリ不足 - キャッシュをクリアして継続\")\n",
    "                optimizer.zero_grad()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # --- バリデーションフェーズ ---\n",
    "    model.eval()\n",
    "    print(f\"[エポック {epoch + 1}] バリデーション中\")\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_metrics = {'rmse': 0.0, 'mae': 0.0, 'psnr': 0.0, 'ssim': 0.0}\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch[\"img\"].to(device)\n",
    "            prior = batch[\"prior\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            # 予測と損失計算\n",
    "            prediction = model(images, prior)\n",
    "            loss = combined_loss(prediction, labels)\n",
    "            \n",
    "            # 損失とメトリクスの蓄積\n",
    "            running_loss += loss.item()\n",
    "            batch_metrics = calculate_metrics(prediction, labels)\n",
    "            \n",
    "            for key in running_metrics:\n",
    "                running_metrics[key] += batch_metrics[key]\n",
    "            \n",
    "            num_batches += 1\n",
    "    \n",
    "    # 平均値の計算\n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_metrics = {k: v / num_batches for k, v in running_metrics.items()}\n",
    "    \n",
    "    # 結果表示\n",
    "    print(f\"[エポック {epoch + 1}] 検証損失: {avg_loss:.6f}, \"\n",
    "          f\"RMSE: {avg_metrics['rmse']:.6f}, MAE: {avg_metrics['mae']:.6f}, \"\n",
    "          f\"PSNR: {avg_metrics['psnr']:.6f}, SSIM: {avg_metrics['ssim']:.6f}\")\n",
    "    \n",
    "    # ベストモデルのチェックと保存\n",
    "    if avg_loss < best_val_loss:\n",
    "        best_val_loss = avg_loss\n",
    "        best_val_metrics = avg_metrics\n",
    "        best_epoch = epoch + 1\n",
    "        \n",
    "        model_save_dir = os.path.join(experiment_dir, TRAINING_CONFIG['model_save_dir'])\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "        \n",
    "        save_model(\n",
    "            model,\n",
    "            epoch + 1,\n",
    "            avg_loss,\n",
    "            model_save_dir,\n",
    "            'nnet_medical_ct_best'\n",
    "        )\n",
    "        print(f\"ベストモデルをエポック {epoch+1} で保存\")\n",
    "    \n",
    "    # ベスト結果の表示\n",
    "    print(f\"[ベスト検証結果] エポック: {best_epoch}, 損失: {best_val_loss:.6f}, \"\n",
    "          f\"RMSE: {best_val_metrics['rmse']:.6f}, MAE: {best_val_metrics['mae']:.6f}\")\n",
    "    \n",
    "    # TensorBoardロギング\n",
    "    if tb_writer:\n",
    "        tb_writer.add_scalar('Val/Loss', avg_loss, epoch)\n",
    "        tb_writer.add_scalar('Val/RMSE', avg_metrics['rmse'], epoch)\n",
    "        tb_writer.add_scalar('Val/MAE', avg_metrics['mae'], epoch)\n",
    "        tb_writer.add_scalar('Val/PSNR', avg_metrics['psnr'], epoch)\n",
    "        tb_writer.add_scalar('Val/SSIM', avg_metrics['ssim'], epoch)\n",
    "        tb_writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "        \n",
    "        # ベストモデルの画像可視化\n",
    "        if epoch > 5 and avg_loss == best_val_loss:\n",
    "            img_cpu = images[0].cpu().float()\n",
    "            prior_cpu = prior[0].cpu().float()\n",
    "            label_cpu = labels[0].cpu().float()\n",
    "            pred_cpu = prediction[0].cpu().float()\n",
    "            \n",
    "            grid = torchvision.utils.make_grid(\n",
    "                [img_cpu, prior_cpu, label_cpu, pred_cpu],\n",
    "                nrow=4,\n",
    "                normalize=True\n",
    "            )\n",
    "            tb_writer.add_image(f\"Validation/Artifact_Removal\", grid, epoch)\n",
    "    \n",
    "    # Weights & Biasesロギング\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            'val_loss': avg_loss,\n",
    "            'val_rmse': avg_metrics['rmse'],\n",
    "            'val_mae': avg_metrics['mae'],\n",
    "            'val_psnr': avg_metrics['psnr'],\n",
    "            'val_ssim': avg_metrics['ssim'],\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "            'epoch': epoch + 1\n",
    "        })\n",
    "\n",
    "print(\"トレーニングが正常に完了しました!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. トレーニング完了・リソース解放\n",
    "\n",
    "**目的**: トレーニング終了後の適切なリソース管理\n",
    "- **ロギング終了**: TensorBoard・W&Bセッションの正常終了\n",
    "- **メモリ解放**: GPU・CPUメモリの効率的な解放\n",
    "- **結果保存**: 最終モデルと実験ログの確認\n",
    "\n",
    "**重要**: 実験結果は `experiment_dir` に自動保存されます\n",
    "- モデル重み: `trained_model/` ディレクトリ\n",
    "- TensorBoard: `tensorboard/` ディレクトリ  \n",
    "- 設定ファイル: `configs/` ディレクトリ\n",
    "- W&Bログ: オンラインダッシュボードで確認可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧹 リソースクリーンアップを実行中...\")\n",
    "\n",
    "# TensorBoardライターの安全な終了\n",
    "if tb_writer:\n",
    "    try:\n",
    "        tb_writer.close()\n",
    "        print(\"✅ TensorBoardライターを正常に終了\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ TensorBoard終了エラー: {str(e)}\")\n",
    "else:\n",
    "    print(\"ℹ️ TensorBoardは使用されていません\")\n",
    "\n",
    "# Weights & Biasesセッションの終了\n",
    "if use_wandb:\n",
    "    try:\n",
    "        wandb.finish()\n",
    "        print(\"✅ Weights & Biasesセッションを正常に終了\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ W&B終了エラー: {str(e)}\")\n",
    "else:\n",
    "    print(\"ℹ️ Weights & Biasesは使用されていません\")\n",
    "\n",
    "# GPUメモリの解放\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        # キャッシュされたメモリを解放\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # GPU使用量の表示\n",
    "        gpu_memory_used = torch.cuda.memory_allocated(device) / 1024**3  # GB\n",
    "        gpu_memory_cached = torch.cuda.memory_reserved(device) / 1024**3  # GB\n",
    "        \n",
    "        print(f\"✅ GPUメモリを解放完了\")\n",
    "        print(f\"  現在の使用量: {gpu_memory_used:.2f} GB\")\n",
    "        print(f\"  キャッシュ済み: {gpu_memory_cached:.2f} GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ GPUメモリ解放エラー: {str(e)}\")\n",
    "else:\n",
    "    print(\"ℹ️ GPUは使用されていません\")\n",
    "\n",
    "# 実験結果の確認とサマリー表示\n",
    "print(f\"\\n📁 実験結果ディレクトリ: {experiment_dir}\")\n",
    "print(f\"📋 実験サマリー:\")\n",
    "\n",
    "try:\n",
    "    # 保存されたファイルの確認\n",
    "    if os.path.exists(experiment_dir):\n",
    "        subdirs = ['tensorboard', 'configs', 'trained_model']\n",
    "        for subdir in subdirs:\n",
    "            subdir_path = os.path.join(experiment_dir, subdir)\n",
    "            if os.path.exists(subdir_path):\n",
    "                file_count = len([f for f in os.listdir(subdir_path) \n",
    "                                if os.path.isfile(os.path.join(subdir_path, f))])\n",
    "                print(f\"  {subdir}/: {file_count} ファイル\")\n",
    "            else:\n",
    "                print(f\"  {subdir}/: 作成されませんでした\")\n",
    "                \n",
    "        print(f\"\\n🎉 トレーニング完了！\")\n",
    "        print(f\"📊 TensorBoard表示: tensorboard --logdir {os.path.join(experiment_dir, 'tensorboard')}\")\n",
    "        \n",
    "        if use_wandb:\n",
    "            print(f\"🌐 W&Bダッシュボード: https://wandb.ai/{LOGGING_CONFIG.get('wandb_entity', 'your-entity')}/{LOGGING_CONFIG.get('wandb_project', 'your-project')}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 実験ディレクトリ確認エラー: {str(e)}\")\n",
    "\n",
    "print(f\"\\n✨ 全ての処理が正常に完了しました！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
